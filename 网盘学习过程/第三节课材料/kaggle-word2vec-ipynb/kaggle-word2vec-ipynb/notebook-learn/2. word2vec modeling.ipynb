{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# word2vec训练词向量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def load_dataset(name, nrows=None):\n",
    "    datasets = {\n",
    "        'unlabeled_train': 'unlabeledTrainData.tsv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test': 'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    data_file = os.path.join('..', 'data', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep='\\t', escapechar='\\\\', nrows=nrows)\n",
    "    print('Number of reviews: {}'.format(len(df)))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读入无标签数据\n",
    "用于训练生成word2vec词向量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": "        id                                             review\n0   9999_0  Watching Time Chasers, it obvious that it was ...\n1  45057_0  I saw this film about 20 years ago and remembe...\n2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n3   7161_0  I went to see this film with a great deal of e...\n4  43971_0  Yes, I agree with everyone on this site this m...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9999_0</td>\n      <td>Watching Time Chasers, it obvious that it was ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>45057_0</td>\n      <td>I saw this film about 20 years ago and remembe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15561_0</td>\n      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7161_0</td>\n      <td>I went to see this film with a great deal of e...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>43971_0</td>\n      <td>Yes, I agree with everyone on this site this m...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('unlabeled_train')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 和第一个笔记本一样做数据的预处理\n",
    "稍稍有一点不一样的是，我们留了个候选，可以去除停用词，也可以不去除停用词"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def print_call_counts(f):\n",
    "    n = 0\n",
    "    def wrapped(*args, **kwargs):\n",
    "        nonlocal n\n",
    "        n += 1\n",
    "        if n % 1000 == 1:\n",
    "            print('method {} called {} times'.format(f.__name__, n))\n",
    "            return f(*args, **kwargs)\n",
    "        return wrapped\n",
    "\n",
    "@print_call_counts\n",
    "def split_sentences(review):\n",
    "    raw_sentences = tokenizer.tokenize(review.str.strip())\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [24]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m sentences \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[43msplit_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreview\u001B[49m\u001B[43m)\u001B[49m, [])\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m reviews -> \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m sentences\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mlen\u001B[39m(df), \u001B[38;5;28mlen\u001B[39m(sentences)))\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "%time\n",
    "sentences = sum(split_sentences(df.review), [])\n",
    "print('{} reviews -> {} sentences'.format(len(df), len(sentences)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreview\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.tokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1272\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, realign_boundaries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1273\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1274\u001B[0m \u001B[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001B[39;00m\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msentences_from_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealign_boundaries\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.sentences_from_text\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, realign_boundaries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1327\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[0;32m   1328\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[0;32m   1329\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[0;32m   1330\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, realign_boundaries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1327\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[0;32m   1328\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[0;32m   1329\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[0;32m   1330\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.span_tokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m realign_boundaries:\n\u001B[0;32m   1321\u001B[0m     slices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_realign_boundaries(text, slices)\n\u001B[1;32m-> 1322\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m slices:\n\u001B[0;32m   1323\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (sentence\u001B[38;5;241m.\u001B[39mstart, sentence\u001B[38;5;241m.\u001B[39mstop)\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._realign_boundaries\u001B[1;34m(self, text, slices)\u001B[0m\n\u001B[0;32m   1408\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1409\u001B[0m \u001B[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001B[39;00m\n\u001B[0;32m   1410\u001B[0m \u001B[38;5;124;03mshould otherwise be included in the same sentence.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1418\u001B[0m \u001B[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001B[39;00m\n\u001B[0;32m   1419\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1420\u001B[0m realign \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1421\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence1, sentence2 \u001B[38;5;129;01min\u001B[39;00m _pair_iter(slices):\n\u001B[0;32m   1422\u001B[0m     sentence1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(sentence1\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m+\u001B[39m realign, sentence1\u001B[38;5;241m.\u001B[39mstop)\n\u001B[0;32m   1423\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sentence2:\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001B[0m, in \u001B[0;36m_pair_iter\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m    316\u001B[0m iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(iterator)\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 318\u001B[0m     prev \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._slices_from_text\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m   1393\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_slices_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text):\n\u001B[0;32m   1394\u001B[0m     last_break \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1395\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m match, context \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_match_potential_end_contexts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1396\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_contains_sentbreak(context):\n\u001B[0;32m   1397\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mslice\u001B[39m(last_break, match\u001B[38;5;241m.\u001B[39mend())\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m   1373\u001B[0m before_words \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m   1374\u001B[0m matches \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m-> 1375\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m match \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lang_vars\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperiod_context_re\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinditer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)):\n\u001B[0;32m   1376\u001B[0m     \u001B[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001B[39;00m\n\u001B[0;32m   1377\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m matches \u001B[38;5;129;01mand\u001B[39;00m match\u001B[38;5;241m.\u001B[39mend() \u001B[38;5;241m>\u001B[39m before_start:\n\u001B[0;32m   1378\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "tokenizer.tokenize(df.review.str.strip())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 用gensim训练词嵌入模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s :%(message)s', level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 设定词向量训练的参数\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Training model...')\n",
    "model = Word2Vec(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}